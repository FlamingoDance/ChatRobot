{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "from utils.file_utils import print_lines, load_lines, load_conversations, extract_sentence_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看原始文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\\n'\n",
      "b'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\\n'\n",
      "b'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\\n'\n",
      "b'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\\n'\n",
      "b\"L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\\n\"\n",
      "b'L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\\n'\n",
      "b\"L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\\n\"\n",
      "b'L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\\n'\n",
      "b'L870 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I\\'m kidding.  You know how sometimes you just become this \"persona\"?  And you don\\'t know how to quit?\\n'\n",
      "b'L869 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Like my fear of wearing pastels?\\n'\n"
     ]
    }
   ],
   "source": [
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "corpus_path = os.path.join(\"data\", corpus_name)\n",
    "movie_lines_path = os.path.join(corpus_path, \"movie_lines.txt\") # 原始文件路径\n",
    "print_lines(movie_lines_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 查看使用 file_utils 中的函数处理好文本后的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus...\n",
      "\n",
      "Loading conversations...\n",
      "\n",
      "Writing newly formatted file...\n",
      "\n",
      "Sample lines from file:\n",
      "b\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\r\\n\"\n",
      "b\"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\r\\n\"\n",
      "b\"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\r\\n\"\n",
      "b\"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\r\\n\"\n",
      "b\"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\r\\n\"\n",
      "b\"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\r\\n\"\n",
      "b\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\r\\n\"\n",
      "b'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\r\\n'\n",
      "b\"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\r\\n\"\n",
      "b'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\r\\n'\n"
     ]
    }
   ],
   "source": [
    "# Define path to new file\n",
    "datafile_path = os.path.join(corpus_path, \"formatted_movie_lines.txt\")\n",
    "\n",
    "delimiter = '\\t'\n",
    "# Unescape the delimiter\n",
    "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
    "\n",
    "# Initialize lines dict, conversations list, and field ids\n",
    "lines = {}\n",
    "conversations = []\n",
    "MOVIE_LINES_FIELDS = [\"lineID\", \"characterID\",\n",
    "                    \"movieID\", \"character\", \"text\"]\n",
    "MOVIE_CONVERSATIONS_FIELDS = [\"character1ID\", \"character2ID\",\n",
    "                            \"movieID\", \"utteranceIDs\"]\n",
    "\n",
    "# Load lines and process conversations\n",
    "print(\"\\nProcessing corpus...\")\n",
    "lines = load_lines(movie_lines_path, MOVIE_LINES_FIELDS)\n",
    "print(\"\\nLoading conversations...\")\n",
    "conversations = load_conversations(os.path.join(corpus_path, \"movie_conversations.txt\"), lines, MOVIE_CONVERSATIONS_FIELDS)\n",
    "\n",
    "# Write new csv file\n",
    "print(\"\\nWriting newly formatted file...\")\n",
    "with open(datafile_path, 'w', encoding='utf-8') as outputfile:\n",
    "    writer = csv.writer(outputfile, delimiter=delimiter,\n",
    "                        lineterminator='\\n')\n",
    "    for pair in extract_sentence_pairs(conversations):\n",
    "        writer.writerow(pair)\n",
    "\n",
    "# Print a sample of lines\n",
    "print(\"\\nSample lines from file:\")\n",
    "print_lines(datafile_path, n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Vocab 类测试\n",
    "\n",
    "在运行一下代码之前，请在这里测试你编写好的 Vocab 类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lorem': 1,\n",
       " 'ipsum': 1,\n",
       " 'dolor': 2,\n",
       " 'sit': 1,\n",
       " 'amet,': 1,\n",
       " 'consectetur': 1,\n",
       " 'adipiscing': 1,\n",
       " 'elit,': 1,\n",
       " 'sed': 1,\n",
       " 'do': 1,\n",
       " 'eiusmod': 1,\n",
       " 'tempor': 1,\n",
       " 'incididunt': 1,\n",
       " 'ut': 2,\n",
       " 'labore': 1,\n",
       " 'et': 1,\n",
       " 'dolore': 2,\n",
       " 'magna': 1,\n",
       " 'aliqua.': 1,\n",
       " 'Ut': 1,\n",
       " 'enim': 1,\n",
       " 'ad': 1,\n",
       " 'minim': 1,\n",
       " 'veniam,': 1,\n",
       " 'quis': 1,\n",
       " 'nostrud': 1,\n",
       " 'exercitation': 1,\n",
       " 'ullamco': 1,\n",
       " 'laboris': 1,\n",
       " 'nisi': 1,\n",
       " 'aliquip': 1,\n",
       " 'ex': 1,\n",
       " 'ea': 1,\n",
       " 'commodo': 1,\n",
       " 'consequat.': 1,\n",
       " 'Duis': 1,\n",
       " 'aute': 1,\n",
       " 'irure': 1,\n",
       " 'in': 3,\n",
       " 'reprehenderit': 1,\n",
       " 'voluptate': 1,\n",
       " 'velit': 1,\n",
       " 'esse': 1,\n",
       " 'cillum': 1,\n",
       " 'eu': 1,\n",
       " 'fugiat': 1,\n",
       " 'nulla': 1,\n",
       " 'pariatur.': 1,\n",
       " 'Excepteur': 1,\n",
       " 'sint': 1,\n",
       " 'occaecat': 1,\n",
       " 'cupidatat': 1,\n",
       " 'non': 1,\n",
       " 'proident,': 1,\n",
       " 'sunt': 1,\n",
       " 'culpa': 1,\n",
       " 'qui': 1,\n",
       " 'officia': 1,\n",
       " 'deserunt': 1,\n",
       " 'mollit': 1,\n",
       " 'anim': 1,\n",
       " 'id': 1,\n",
       " 'est': 1,\n",
       " 'laborum.': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# limsum，待测试句子\n",
    "from utils.vocab import Vocab\n",
    "limsum = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\"\n",
    "vocab = Vocab(\"Lorem ipsum\")\n",
    "vocab.add_sentence(limsum)\n",
    "vocab.word2count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成单词词典\n",
    "\n",
    "这里可能会运行十几秒，耐心等待。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已获取大小为18004的vocab,长度为64271的pairs\n",
      "Counted words: 18007\n"
     ]
    }
   ],
   "source": [
    "from utils.data_utils import load_prepare_data\n",
    "\n",
    "# [TODO][Optional] 如果实现了硬盘保存，文件会保存的位置\n",
    "save_dir = os.path.join(\"data\", corpus_name)\n",
    "# 载入/处理 词典 和 对话文本对\n",
    "vocab, pairs = load_prepare_data(corpus_name, datafile_path, save_dir)\n",
    "print(\"Counted words:\", vocab.num_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pairs:\n",
      "['there .', 'where ?']\n",
      "['you have my word . as a gentleman', 'you re sweet .']\n",
      "['hi .', 'looks like things worked out tonight huh ?']\n",
      "['you know chastity ?', 'i believe we share an art instructor']\n",
      "['have fun tonight ?', 'tons']\n",
      "['well no . . .', 'then that s all you had to say .']\n",
      "['then that s all you had to say .', 'but']\n",
      "['but', 'you always been this selfish ?']\n",
      "['do you listen to this crap ?', 'what crap ?']\n",
      "['what good stuff ?', 'the real you .']\n"
     ]
    }
   ],
   "source": [
    "# 检验，打印前10个对话文本对\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['name', 'trimmed', 'word2index', 'word2count', 'index2word', 'num_words'])\n",
      "vocab.word2count:\n",
      "there 2013\n",
      ". 104124\n",
      "where 2475\n",
      "? 43942\n",
      "you 29248\n",
      "have 3023\n",
      "my 3148\n",
      "word 125\n",
      "as 558\n",
      "a 8579\n",
      "vocab.index2word:\n",
      "3 there\n",
      "4 .\n",
      "5 where\n",
      "6 ?\n",
      "7 you\n",
      "8 have\n",
      "9 my\n",
      "10 word\n",
      "11 as\n",
      "12 a\n"
     ]
    }
   ],
   "source": [
    "# 检验，查看 vocab 的属性/成员变量\n",
    "print(vocab.__dict__.keys())\n",
    "# 查看 word2count 的前10项\n",
    "print(\"vocab.word2count:\")\n",
    "for i, (key, value) in enumerate(vocab.word2count.items()):\n",
    "    print(key, value)\n",
    "    if i >= 10-1:\n",
    "        break\n",
    "# 查看 index2word 的前10项\n",
    "print(\"vocab.index2word:\")\n",
    "for i, (key, value) in enumerate(vocab.index2word.items()):\n",
    "    print(key, value)\n",
    "    if i >= 10-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 测试 trim_rare_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed from 64271 pairs to 53125, 0.8266 of total\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocessing import trim_rare_words, MIN_COUNT\n",
    "pairs = trim_rare_words(vocab, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import indices_from_sentence, zero_padding, binary_matrix\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "# [random.choice(pairs) for _ in range(small_batch_size)]\n",
    "chosen_pairs = [pairs[_] for _ in range(small_batch_size)]\n",
    "input_batch, output_batch = [], []\n",
    "for pair in chosen_pairs:\n",
    "    input_batch.append(pair[0])\n",
    "    output_batch.append(pair[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there .', 'you have my word . as a gentleman', 'hi .', 'have fun tonight ?', 'well no . . .']\n",
      "['where ?', 'you re sweet .', 'looks like things worked out tonight huh ?', 'tons', 'then that s all you had to say .']\n"
     ]
    }
   ],
   "source": [
    "print(input_batch)\n",
    "print(output_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查 input_batch 的 indices_batch 和 padded_list 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_batch: [[3, 4, 2], [7, 8, 9, 10, 4, 11, 12, 13, 2], [16, 4, 2], [8, 31, 22, 6, 2], [33, 34, 4, 4, 4, 2]]\n",
      "padded_list: [(3, 7, 16, 8, 33), (4, 8, 4, 31, 34), (2, 9, 2, 22, 4), (0, 10, 0, 6, 4), (0, 4, 0, 2, 4), (0, 11, 0, 0, 2), (0, 12, 0, 0, 0), (0, 13, 0, 0, 0), (0, 2, 0, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "indices_batch = [indices_from_sentence(\n",
    "    vocab, sentence) for sentence in input_batch]\n",
    "print(f\"indices_batch: {indices_batch}\")\n",
    "padded_list = zero_padding(indices_batch)\n",
    "print(f\"padded_list: {padded_list}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查 output_batch 的 indices_batch 和 padded_list 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices_batch: [[5, 6, 2], [7, 14, 15, 4, 2], [17, 18, 19, 20, 21, 22, 23, 6, 2], [32, 2], [35, 36, 37, 38, 7, 39, 40, 41, 4, 2]]\n",
      "padded_list: [(5, 7, 17, 32, 35), (6, 14, 18, 2, 36), (2, 15, 19, 0, 37), (0, 4, 20, 0, 38), (0, 2, 21, 0, 7), (0, 0, 22, 0, 39), (0, 0, 23, 0, 40), (0, 0, 6, 0, 41), (0, 0, 2, 0, 4), (0, 0, 0, 0, 2)]\n",
      "(5, 7, 17, 32, 35)\n",
      "(6, 14, 18, 2, 36)\n",
      "(2, 15, 19, 0, 37)\n",
      "(0, 4, 20, 0, 38)\n",
      "(0, 2, 21, 0, 7)\n",
      "(0, 0, 22, 0, 39)\n",
      "(0, 0, 23, 0, 40)\n",
      "(0, 0, 6, 0, 41)\n",
      "(0, 0, 2, 0, 4)\n",
      "(0, 0, 0, 0, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_batch = [indices_from_sentence(\n",
    "    vocab, sentence) for sentence in output_batch]\n",
    "print(f\"indices_batch: {indices_batch}\")\n",
    "padded_list = zero_padding(indices_batch)\n",
    "print(f\"padded_list: {padded_list}\")\n",
    "[print(_) for _ in padded_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查 binary_matrix 函数的输出 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 0, 1]\n",
      "[0, 1, 1, 0, 1]\n",
      "[0, 1, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 1, 0, 1]\n",
      "[0, 0, 0, 0, 1]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "mask = binary_matrix(padded_list)\n",
    "[print(_) for _ in mask]\n",
    "print(type(mask))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 检查 binary_matrix 函数的输出 mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.preprocessing import input_variable, output_variable\n",
    "input_, lengths = input_variable(input_batch, vocab)\n",
    "target, mask, max_target_len = output_variable(output_batch, vocab)\n",
    "max_target_len\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_:\n",
      "tensor([[ 3,  7, 16,  8, 33],\n",
      "        [ 4,  8,  4, 31, 34],\n",
      "        [ 2,  9,  2, 22,  4],\n",
      "        [ 0, 10,  0,  6,  4],\n",
      "        [ 0,  4,  0,  2,  4],\n",
      "        [ 0, 11,  0,  0,  2],\n",
      "        [ 0, 12,  0,  0,  0],\n",
      "        [ 0, 13,  0,  0,  0],\n",
      "        [ 0,  2,  0,  0,  0]])\n",
      "target:\n",
      "tensor([[ 5,  7, 17, 32, 35],\n",
      "        [ 6, 14, 18,  2, 36],\n",
      "        [ 2, 15, 19,  0, 37],\n",
      "        [ 0,  4, 20,  0, 38],\n",
      "        [ 0,  2, 21,  0,  7],\n",
      "        [ 0,  0, 22,  0, 39],\n",
      "        [ 0,  0, 23,  0, 40],\n",
      "        [ 0,  0,  6,  0, 41],\n",
      "        [ 0,  0,  2,  0,  4],\n",
      "        [ 0,  0,  0,  0,  2]])\n",
      "lengths:\n",
      "tensor([3, 9, 3, 5, 6])\n",
      "mask:\n",
      "tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True, False,  True],\n",
      "        [False,  True,  True, False,  True],\n",
      "        [False,  True,  True, False,  True],\n",
      "        [False, False,  True, False,  True],\n",
      "        [False, False,  True, False,  True],\n",
      "        [False, False,  True, False,  True],\n",
      "        [False, False,  True, False,  True],\n",
      "        [False, False, False, False,  True]])\n",
      "max_target_len: \n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(f\"input_:\\n{input_}\")\n",
    "print(f\"target:\\n{target}\")\n",
    "print(f\"lengths:\\n{lengths}\")\n",
    "print(f\"mask:\\n{mask}\")\n",
    "print(f\"max_target_len: \\n{max_target_len}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2, 4, 5, 6] [2, 4, 5] [1, 2]\n",
      "[[1, 3, 2, 4, 5, 6], [2, 4, 5], [1, 2]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1, 2, 1), (3, 4, 2), (2, 5, 0), (4, 0, 0), (5, 0, 0), (6, 0, 0)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "l = [[1,3,2,4,5,6],[2,4,5],[1,2]]\n",
    "print(*l)\n",
    "print(l)\n",
    "list(itertools.zip_longest(*l,fillvalue=0))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdd02df4a038e808130b65f74f3963603a613d35b6d5a8e2df3811214e297985"
  },
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "general"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
